{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6795f00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.1.min.js\", \"https://cdn.holoviz.org/panel/1.2.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import requests as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "#from osgeo import gdal\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.shutil import copy\n",
    "import pyproj\n",
    "from pyproj import Proj\n",
    "from shapely.ops import transform\n",
    "import xarray as xr\n",
    "#import geoviews as gv\n",
    "#from cartopy import crs\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import boto3\n",
    "from rasterio.session import AWSSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d92fb1",
   "metadata": {},
   "source": [
    "# Section 1: Navigating the CMR-STAC API\n",
    "\n",
    "## Set GDAL Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7f9834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rasterio.env.Env at 0x1fb16a3c410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.packages.urllib3.disable_warnings()\n",
    "\n",
    "def get_temp_creds():\n",
    "    temp_creds_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n",
    "    return r.get(temp_creds_url, verify=False).json()\n",
    "\n",
    "temp_creds_req = get_temp_creds()\n",
    "#temp_creds_req\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')\n",
    "\n",
    "rio_env = rio.Env(AWSSession(session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  CPL_VSIL_CURL_ALLOWED_EXTENSIONS='tif',\n",
    "                  GDAL_HTTP_UNSAFESSL='YES',\n",
    "                  VSI_CACHE=True,\n",
    "                  region_name='us-west-2',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df9319",
   "metadata": {},
   "source": [
    "## 1. STAC API: Endpoint that enables the querying of STAC items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f106eb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "stac_version\n",
      "type\n",
      "description\n",
      "links\n",
      "You are now using the stac API (STAC Version: 1.0.0). This is the landing page for CMR-STAC. Each provider link contains a STAC endpoint.\n",
      "There are 59 STAC catalogs available in CMR.\n"
     ]
    }
   ],
   "source": [
    "stac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\n",
    "r.packages.urllib3.disable_warnings()  # Disable SSL warnings\n",
    "stac_response = r.get(stac, verify=False).json() # Call the STAC API endpoint\n",
    "for s in stac_response: print(s)\n",
    "\n",
    "print(f\"You are now using the {stac_response['id']} API (STAC Version: {stac_response['stac_version']}). {stac_response['description']}\")\n",
    "print(f\"There are {len(stac_response['links'])} STAC catalogs available in CMR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9325d",
   "metadata": {},
   "source": [
    "## 2. STAC Catalog: Contains a JSON file of links that organize all of the collections available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9280be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: LPCLOUD\n",
      "title: LPCLOUD\n",
      "description: Root catalog for LPCLOUD\n",
      "type: Catalog\n",
      "stac_version: 1.0.0\n",
      "links: [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Provider catalog', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'collections', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections', 'title': 'Provider Collections', 'type': 'application/json'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'GET'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'POST'}, {'rel': 'conformance', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance', 'title': 'Conformance Classes', 'type': 'application/geo+json'}, {'rel': 'service-desc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml', 'title': 'OpenAPI Doc', 'type': 'application/vnd.oai.openapi;version=3.0'}, {'rel': 'service-doc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/index.html', 'title': 'HTML documentation', 'type': 'text/html'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM_NUMNC.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM_NC.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/WaterBalance_Daily_Historical_GRIDMET.v1.5', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2G_CLOUD.v002', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2G_LSTE.v002', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1CG_RAD.v002', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_ATT.v002', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2_CLOUD.v002', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_GEO.v002', 'type': 'application/json'}, {'rel': 'next', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD?page=2'}]\n",
      "conformsTo: ['https://api.stacspec.org/v1.0.0-beta.1/core', 'https://api.stacspec.org/v1.0.0-beta.1/item-search', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#fields', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#query', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#sort', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#context', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/core', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/oas30', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/geojson']\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD is the Provider catalog\n",
      "https://cmr.earthdata.nasa.gov/stac/ is the Root catalog\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections is the Provider Collections\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance is the Conformance Classes\n",
      "https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml is the OpenAPI Doc\n",
      "https://api.stacspec.org/v1.0.0-beta.1/index.html is the HTML documentation\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM_NUMNC.v003\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM_NC.v003\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/WaterBalance_Daily_Historical_GRIDMET.v1.5\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2G_CLOUD.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2G_LSTE.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1CG_RAD.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_ATT.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2_CLOUD.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_GEO.v002\n",
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD?page=2\n"
     ]
    }
   ],
   "source": [
    "stac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  # Search for only LP-specific catalogs\n",
    "\n",
    "# LPCLOUD is the STAC catalog we will be using and exploring today\n",
    "lp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href'], verify=False).json()\n",
    "for l in lp_cloud: print(f\"{l}: {lp_cloud[l]}\")\n",
    "\n",
    "# Print the links contained in the LP CLOUD STAC Catalog\n",
    "lp_links = lp_cloud['links']\n",
    "for l in lp_links: \n",
    "    try: \n",
    "        print(f\"{l['href']} is the {l['title']}\")\n",
    "    except:\n",
    "        print(f\"{l['href']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9899ea",
   "metadata": {},
   "source": [
    "## 3. STAC Collection: Extension of STAC Catalog containing additional information that describe the STAC items in that collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a86501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections\n",
      "{'id': 'LPCLOUD', 'stac_version': '1.0.0', 'description': 'All collections provided by LPCLOUD', 'license': 'not-provided', 'type': 'Catalog', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections', 'title': 'All collections provided by LPCLOUD', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/', 'title': 'CMR-STAC Root', 'type': 'application/json'}, {'rel': 'prev', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/?page=1'}, {'rel': 'next', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/?page=3'}], 'collections': [{'id': 'ECO_L2_LSTE.v002', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002', 'type': 'Collection', 'description': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data globally as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\r\\nThe ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m (ECO_L2_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2_LSTE data product was derived using a physics-based Temperature and Emissivity Separation (TES) algorithm. The ECO_L2_LSTE is provided as swath data and has a spatial resolution of 70 meters (m). The corresponding  ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_LSTE data product.\\r\\nThe ECO_L2_LSTE Version 2 data product contains layers of LST, emissivity for bands 1 through 5, quality control for LST&E, LST error, emissivity error for bands 1 through 5, wideband emissivity, Precipitable Water Vapor (PWV), cloud mask, and water mask. \\r\\nFor acquisitions between May 15, 2019, and April 28, 2023, data products contain data values for TIR bands 2, 4 and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS. \\r\\n\\r\\nKnown Issues: *Data acquisition gap: ECOSTRESS was launched on June 29, 2018, and moved to autonomous science operations on August 20, 2018, following a successful in-orbit checkout period. On September 29, 2018, ECOSTRESS experienced an anomaly with its primary mass storage unit (MSU). ECOSTRESS has a primary and secondary MSU (A and B). On December 5, 2018, the instrument was switched to the secondary MSU and science operations resumed. On March 14, 2019, the secondary MSU experienced a similar anomaly, temporarily halting science acquisitions. On May 15, 2019, a new data acquisition approach was implemented, and science acquisitions resumed. To optimize the new acquisition approach TIR bands 2, 4, and 5 are being downloaded. The data products are as previously, except the bands not downloaded contain fill values (L1 radiance and L2 emissivity). This approach was implemented from May 15, 2019, through April 28, 2023.\\r\\n*Data acquisition gap: From February 8 to February 16, 2020, an ECOSTRESS instrument issue resulted in a data anomaly that created striping in band 4 (10.5 micron). These data products have been reprocessed and are available for download. No ECOSTRESS data were acquired on February 17, 2020, due to the instrument being in SAFEHOLD. Data acquired following the anomaly have not been affected.\\r\\n*Data acquisition: ECOSTRESS has now successfully returned to 5-band mode after being in 3-band mode since 2019. This feature was successfully enabled following a Data Processing Unit firmware update (version 4.1) to the payload on April 28, 2023. To better balance contiguous science data scene variables, 3-band collection is currently being interleaved with 5-band acquisitions over the orbital day/night periods.\\r\\n', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2_LSTE.v002', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2_LSTE.v002/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076114664-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076114664-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2018-07-09T00:00:00.000Z', None]]}}}, {'id': 'ECO_L1B_RAD.v002', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m V002', 'type': 'Collection', 'description': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data globally as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\r\\nThe ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m (ECO_L1B_RAD) Version 2 data product provides at-sensor calibrated radiance values retrieved for five thermal infrared (TIR) bands operating between 8 and 12.5 µm. Additionally, the digital numbers (DN) for the shortwave infrared (SWIR) band are provided. The TIR bands are spatially co-registered to produce a variable spatial resolution between 70 meters (m) and 90 m at the edge of the swath. The ECO_L1B_RAD data product is provided as swath data and does not contain geolocation information. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L1B_RAD data product. The geographic coverage of acquisitions for the ECO_L1B_RAD Version 2 data product extends to areas outside of those indicated on the coverage map. \\r\\nThe ECO_L1B_RAD Version 2 data product contains layers of radiance values for the five TIR bands, DN values for the SWIR band, associated data quality indicators, and ancillary data distributed in HDF5 format. For acquisitions after May 15, 2019, data products contain data values for the 8.785 μm, 10.522 μm, and 12.001 μm (TIR) bands only. The 1.6 μm (SWIR), 8.285 μm (TIR), and 9.060 μm (TIR) bands contain fill values to accommodate direct streaming of data from the ISS.\\r\\n\\r\\nKnown Issues: *Cannot perform spatial query on ECO_L1B_RAD in NASA Earthdata Search: ECO_L1B_RAD does not contain spatial attributes, so granules cannot be searched by geographic location. Users should search for ECO_L1B_RAD data products by orbit number instead.\\r\\n*Data acquisition gap: ECOSTRESS was launched on June 29, 2018, and moved to autonomous science operations on August 20, 2018, following a successful in-orbit checkout period. On September 29, 2018, ECOSTRESS experienced an anomaly with its primary mass storage unit (MSU). ECOSTRESS has a primary and secondary MSU (A and B). On December 5, 2018, the instrument was switched to the secondary MSU and science operations resumed. On March 14, 2019, the secondary MSU experienced a similar anomaly, temporarily halting science acquisitions. On May 15, 2019, a new data acquisition approach was implemented, and science acquisitions resumed. To optimize the new acquisition approach TIR bands 2, 4 and 5 are being downloaded. The data products are as previously, except the bands not downloaded contain fill values (L1 radiance and L2 emissivity). This approach was implemented from May 15, 2019, through April 28, 2023.\\r\\n*Data acquisition gap: From February 8 to February 16, 2020, an ECOSTRESS instrument issue resulted in a data anomaly that created striping in band 4 (10.5 micron). These data products have been reprocessed and are available for download. No ECOSTRESS data were acquired on February 17, 2020, due to the instrument being in SAFEHOLD. Data acquired following the anomaly have not been affected.\\r\\n*Missing scan data/striping features: During testing, an instrument artifact was encountered in ECOSTRESS bands 1 and 5, resulting in missing values. A machine learning algorithm has been applied to interpolate missing values. For more information on the missing scan filling techniques and outcomes, see Section 3.3.2 of the User Guide.\\r\\n*Scan overlap: An overlap between ECOSTRESS scans results in a clear line overlap and repeating data. Additional information is available in Section 3.2 of the User Guide. \\r\\n*Scan flipping: Improvements to the visualization of the data to compensate for instrument orientation are discussed in Section 3.4 of the User Guide. \\r\\n*Data acquisition: ECOSTRESS has now successfully returned to 5-band mode after being in 3-band mode since 2019. This feature was successfully enabled following a Data Processing Unit firmware update (version 4.1) to the payload on April 28, 2023. To better balance contiguous science data scene variables, 3-band collection is currently being interleaved with 5-band acquisitions over the orbital day/night periods.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_RAD.v002', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1B_RAD.v002/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076116385-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076116385-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2018-07-09T00:00:00.000Z', None]]}}}, {'id': 'ECO_L2T_LSTE.v002', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002', 'type': 'Collection', 'description': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data globally as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\r\\nThe ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. This tiled data product is subset from the ECO_L2G_LSTE data product using a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\\r\\nThe ECO_L2T_LSTE Version 2 data product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. This product contains seven layers including LST, LST error, wideband emissivity, quality flags, height, and cloud and water masks.\\r\\n\\r\\nKnown Issues: *Data acquisition gap: ECOSTRESS was launched on June 29, 2018, and moved to autonomous science operations on August 20, 2018, following a successful in-orbit checkout period. On September 29, 2018, ECOSTRESS experienced an anomaly with its primary mass storage unit (MSU). ECOSTRESS has a primary and secondary MSU (A and B). On December 5, 2018, the instrument was switched to the secondary MSU and science operations resumed. On March 14, 2019, the secondary MSU experienced a similar anomaly, temporarily halting science acquisitions. On May 15, 2019, a new data acquisition approach was implemented, and science acquisitions resumed. To optimize the new acquisition approach TIR bands 2, 4, and 5 are being downloaded. The data products are as previously, except the bands not downloaded contain fill values (L1 radiance and L2 emissivity). This approach was implemented from May 15, 2019, through April 28, 2023.\\r\\n*Data acquisition gap: From February 8 to February 16, 2020, an ECOSTRESS instrument issue resulted in a data anomaly that created striping in band 4 (10.5 micron). These data products have been reprocessed and are available for download. No ECOSTRESS data were acquired on February 17, 2020, due to the instrument being in SAFEHOLD. Data acquired following the anomaly have not been affected.\\r\\n*Data acquisition: ECOSTRESS has now successfully returned to 5-band mode after being in 3-band mode since 2019. This feature was successfully enabled following a Data Processing Unit firmware update (version 4.1) to the payload on April 28, 2023. To better balance contiguous science data scene variables, 3-band collection is currently being interleaved with 5-band acquisitions over the orbital day/night periods.\\r\\n* The User Guide for this product lists a view_zenith layer which is not included in the current version of this product.\\r\\n', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2T_LSTE.v002', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L2T_LSTE.v002/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076090826-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2076090826-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2018-07-09T00:00:00.000Z', None]]}}}, {'id': 'ECO_L1CT_RAD.v002', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'ECOSTRESS Tiled Top of Atmosphere Calibrated Radiance Instantaneous L1C Global 70 m V002', 'type': 'Collection', 'description': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data globally between 52° N and 52° S latitudes. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\r\\nThe ECOSTRESS Tiled Top of Atmosphere Calibrated Radiance Instantaneous Level 1 Global 70 m (ECO_L1CT_RAD) Version 2 data product provides at-sensor calibrated radiance values retrieved for five thermal infrared (TIR) bands operating between 8 and 12.5 µm. This tiled data product is generated from the ECO_L1CG_RAD (https://doi.org/10.5067/ECOSTRESS/ECO_L1CG_RAD.002) Version 2 data product using a modified version of the Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/), which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution. Each ECOSTRESS pixel can be assumed to remain at the same location at each timestep within a tile.\\r\\nThe ECO_L1CT_RAD Version 2 data product contains 12 layers distributed in Cloud Optimized GeoTIFF (COG) format consisting of separate files containing five TIR bands, associated data quality indicators, and cloud and water masks. \\r\\n\\r\\nKnown Issues: *Data acquisition gap: ECOSTRESS was launched on June 29, 2018, and moved to autonomous science operations on August 20, 2018, following a successful in-orbit checkout period. On September 29, 2018, ECOSTRESS experienced an anomaly with its primary mass storage unit (MSU). ECOSTRESS has a primary and secondary MSU (A and B). On December 5, 2018, the instrument was switched to the secondary MSU and science operations resumed. On March 14, 2019, the secondary MSU experienced a similar anomaly, temporarily halting science acquisitions. On May 15, 2019, a new data acquisition approach was implemented, and science acquisitions resumed. To optimize the new acquisition approach TIR bands 2, 4, and 5 are being downloaded. The data products are as previously, except the bands not downloaded contain fill values (L1 radiance and L2 emissivity). This approach was implemented from May 15, 2019, through April 28, 2023.\\r\\n*Data acquisition gap: From February 8 to February 16, 2020, an ECOSTRESS instrument issue resulted in a data anomaly that created striping in band 4 (10.5 micron). These data products have been reprocessed and are available for download. No ECOSTRESS data were acquired on February 17, 2020, due to the instrument being in SAFEHOLD. Data acquired following the anomaly have not been affected.\\r\\n*Missing scan data/striping features: During testing, an instrument artifact was encountered in ECOSTRESS bands 1 and 5, resulting in missing values. A machine learning algorithm has been applied to interpolate missing values. For more information on the missing scan filling techniques and outcomes, see Section 3.3.2 of the ECO_L1B_RAD User Guide.\\r\\n*Scan overlap: An overlap between ECOSTRESS scans results in a clear line overlap and repeating data. Additional information is available in Section 3.2 of the ECO_L1B_RAD User Guide.\\r\\n*Scan flipping: Improvements to the visualization of the data to compensate for instrument orientation are discussed in Section 3.4 of the ECO_L1B_RAD User Guide.\\r\\n*Data acquisition: ECOSTRESS has now successfully returned to 5-band mode after being in 3-band mode since 2019. This feature was successfully enabled following a Data Processing Unit firmware update (version 4.1) to the payload on April 28, 2023. To better balance contiguous science data scene variables, 3-band collection is currently being interleaved with 5-band acquisitions over the orbital day/night periods.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1CT_RAD.v002', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ECO_L1CT_RAD.v002/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2595678301-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2595678301-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2018-07-09T00:00:00.000Z', None]]}}}, {'id': 'EMITL1BRAD.v001', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'EMIT L1B At-Sensor Calibrated Radiance and Geolocation Data 60 m V001', 'type': 'Collection', 'description': 'The Earth Surface Mineral Dust Source Investigation (EMIT) instrument measures surface mineralogy, targeting the Earth’s arid dust source regions. EMIT is installed on the International Space Station (ISS). During its one-year mission, EMIT will take mineralogical measurements of  sunlit regions of interest between 52° N latitude and 52° S latitude. A map of the regions being investigated can be found on the EMIT website.\\r\\nThe EMIT Level 1B At-Sensor Calibrated Radiance and Geolocation (EMITL1BRAD) Version 1 data product provides at-sensor calibrated radiance values along with observation data in a spatially raw, non-orthocorrected format. Each EMITL1BRAD granule consists of two Network Common Data Format 4 (NetCDF4) files at a spatial resolution of 60 meters (m): Radiance (EMIT_L1B_RAD) and Observation (EMIT_L1B_OBS). The Radiance file contains the at-sensor radiance measurements of 285 bands with a spectral range of 381-2493 nanometers (nm) and with a spectral resolution of ~7.5 nm, which are held within a single science dataset layer (SDS). The Observation file contains viewing and solar geometries, timing, topographic, and other information related to the observation. \\r\\nEach NetCDF4 file holds a location group containing geometric lookup tables (GLT), which are orthorectified images that provide relative x and y reference locations from the raw scene to allow for projection of the data. Along with the GLT layers, the files also contain latitude, longitude, and elevation layers. The latitude and longitude coordinates are presented using the World Geodetic System 84 (WGS84) ellipsoid. The elevation data was obtained from Shuttle Radar Topography Mission v3 (SRTM v3) data and resampled to EMIT’s spatial resolution.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL1BRAD.v001', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL1BRAD.v001/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408009906-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408009906-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2022-08-09T00:00:00.000Z', None]]}}}, {'id': 'EMITL1BATT.v001', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'EMIT L1B Corrected Spacecraft Attitude and Ephemeris V001', 'type': 'Collection', 'description': 'The Earth Surface Mineral Dust Source Investigation (EMIT) instrument measures surface mineralogy, targeting the Earth’s arid dust source regions. EMIT is installed on the International Space Station (ISS). During its one-year mission, EMIT will take mineralogical measurements of the sunlit regions of interest between 52° N latitude and 52° S latitude. A map of the regions being investigated can be found on the EMIT website. The EMIT Level 1B Corrected Spacecraft Attitude and Ephemeris (EMITL1BATT) Version 1 data product provides both corrected and uncorrected attitude quaternions and spacecraft ephemeris data obtained from the ISS, including Earth-centered inertial (ECI) position and velocity, and associated time elements. The data are provided in 1 second intervals, and each product file contains vectors from the duration of the orbit. The time elements are copied from the ISS raw data. The data for each EMITL1BATT granule are delivered in a single Network Common Data Format 4 (NetCDF4) file.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL1BATT.v001', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL1BATT.v001/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408031090-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408031090-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2022-08-09T00:00:00.000Z', None]]}}}, {'id': 'EMITL2ARFL.v001', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'EMIT L2A Estimated Surface Reflectance and Uncertainty and Masks 60 m V001', 'type': 'Collection', 'description': 'The Earth Surface Mineral Dust Source Investigation (EMIT) instrument measures surface mineralogy, targeting the Earth’s arid dust source regions. EMIT is installed on the International Space Station (ISS). During its one-year mission, EMIT will take mineralogical measurements of sunlit regions of interest between 52° N latitude and 52° S latitude. A map of the regions being investigated can be found on the EMIT website (https://earth.jpl.nasa.gov/emit/instrument/operations/).\\r\\n\\r\\nThe EMIT Level 2A Estimated Surface Reflectance and Uncertainty and Masks (EMITL2ARFL) Version 1 data product provides surface reflectance data in a spatially raw, non-orthocorrected format. Each EMITL2ARFL granule consists of three Network Common Data Format 4 (NetCDF4) files at a spatial resolution of 60 meters (m): Reflectance (EMIT_L2A_RFL), Reflectance Uncertainty (EMIT_L2A_RFLUNCERT), and Reflectance Mask (EMIT_L2A_MASK). The Reflectance file contains surface reflectance maps of 285 bands with a spectral range of 381-2493 nanometers (nm) at a spectral resolution of ~7.5 nm, which are held within a single science dataset layer (SDS). The Reflectance Uncertainty file contains uncertainty estimates about the reflectance captured as per-pixel, per-band, posterior standard deviations. The Reflectance Mask file contains six binary flag bands and two data bands. The binary flag bands identify the presence of features including clouds, water, and spacecraft which indicate if a pixel should be excluded from analysis. The data bands contain estimates of aerosol optical depth (AOD) and water vapor.\\r\\n\\r\\nEach NetCDF4 file holds a location group containing a geometric lookup table (GLT) which is an orthorectified image that provides relative x and y reference locations from the raw scene to allow for projection of the data. Along with the GLT layers, the files will also contain latitude, longitude, and elevation layers. The latitude and longitude coordinates are presented using the World Geodetic System 84 (WGS84) ellipsoid. The elevation data was obtained from Shuttle Radar Topography Mission v3 (SRTM v3) data and resampled to EMIT’s spatial resolution.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL2ARFL.v001', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL2ARFL.v001/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408750690-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408750690-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2022-08-09T00:00:00.000Z', None]]}}}, {'id': 'EMITL2BMIN.v001', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'EMIT L2B Estimated Mineral Identification and Band Depth and Uncertainty 60 m V001', 'type': 'Collection', 'description': 'The Earth Surface Mineral Dust Source Investigation (EMIT) instrument measures surface mineralogy, targeting the Earth’s arid dust source regions. EMIT is installed on the International Space Station (ISS). During its one-year mission, EMIT will take mineralogical measurements of the sunlit regions of interest between 52° N latitude and 52° S latitude. An interactive map showing the regions being investigated, current and forecasted data coverage, and additional data resources can be found on the VSWIR Imaging Spectroscopy Interface for Open Science (VISIONS) EMIT Open Data Portal.  \\r\\n  \\r\\nThe EMIT Level 2B Estimated Mineral Identification and Band Depth and Uncertainty (EMITL2BMIN) Version 1 data product provides estimated mineral identification and band depths in a spatially raw, non-orthocorrected format. Each EMITL2BMIN granule contains two Network Common Data Format 4 (NetCDF4) files at a spatial resolution of 60 meters (m): Mineral Identification (EMIT_L2B_MIN) and Mineral Uncertainty (EMIT_L2B_MINUNCERT). The EMIT_L2B_MIN file contains the band depth (the depth of the identified spectral feature) and the identified mineral for each pixel. Two spectral groups, which correspond to different regions of the spectra, are identified independently and often co-occur. These estimates are generated using the Tetracorder system (code) and are based on EMITL2ARFL reflectance values. The EMIT_L2B_MINUNCERT file provides band depth uncertainty estimates calculated using surface Reflectance Uncertainty values from the EMITL2ARFL data product. The band depth uncertainties are presented as standard deviations. The fit score for each mineral identification is also provided as the coefficient of determination (r2) of the match between the continuum normalized library reference and the continuum normalized observed spectrum. Associated metadata indicates the name and reference information for each identified mineral, and additional information about aggregating minerals into different categories is available in the emit-sds-l2b repository and will be available as subsequent data products.  \\r\\n  \\r\\nThe EMITL2BMIN data product includes a total of 19 Science Dataset (SDS) layers. There are four layers for each of the Spectral Groups (Group 1 and Group 2): Mineral Identification, Band Depth, Band Depth Uncertainties, and Fit Score. Additional layers consist of geometric lookup table (GLT) x values, GLT y values, latitude, longitude, elevation, associated spectral library record, mineral name, URL for the spectral library description, spectral group, spectral library, and spectral group index. A browse image with Group 1 Band Depth, Group 2 Band Depth, Group 1 Band Depth Uncertainty, and Group 2 Band Depth Uncertainty is also included.  \\r\\n  \\r\\nDisclaimer  \\r\\nThis product is generated to support the EMIT mission objectives of constraining the sign of dust related radiative forcing. Ten mineral types are the core focus of this work: Calcite, Chlorite, Dolomite, Goethite, Gypsum, Hematite, Illite+Muscovite, Kaolinite, Montmorillonite, and Vermiculite. A future product will aggregate these results for use in Earth System Models. Additional minerals are included in this product for transparency but were not the focus of this product. Further validation is required to use these additional mineral maps, particularly in the case of resource exploration. Similarly, the separation of minerals with similar spectral features, such as a fine-grained goethite and hematite, is an area of active research. The results presented here are an initial offering, but the precise categorization is likely to evolve over time, and the limits of what can and cannot be separated on the global scale is still being explored. The user is encouraged to read the Algorithm Theoretical Basis Document (ATBD) for more details.\\r\\n', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL2BMIN.v001', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/EMITL2BMIN.v001/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408034484-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2408034484-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -54, 180, 54]]}, 'temporal': {'interval': [['2022-08-09T00:00:00.000Z', None]]}}}, {'id': 'HLSL30.v2.0', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0', 'type': 'Collection', 'description': 'The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from a virtual constellation of satellite sensors. The Operational Land Imager (OLI) is housed aboard the joint NASA/USGS Landsat 8 and Landsat 9 satellites, while the Multi-Spectral Instrument (MSI) is mounted aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8/9 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS)(https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]}, 'temporal': {'interval': [['2013-04-11T00:00:00.000Z', None]]}}}, {'id': 'HLSS30.v2.0', 'stac_version': '1.0.0', 'license': 'not-provided', 'title': 'HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0', 'type': 'Collection', 'description': 'The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. \\r\\n\\r\\nThe HLSS30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSS30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. There are 13 bands included in the HLSS30 product along with four angle bands and a quality assessment (QA) band. See the User Guide for a more detailed description of the individual bands provided in the HLSS30 product.\\r\\n', 'links': [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v2.0', 'title': 'Info about this collection', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'parent', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Parent catalog', 'type': 'application/json'}, {'rel': 'items', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v2.0/items', 'title': 'Granules in this collection', 'type': 'application/json'}, {'rel': 'about', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957295-LPCLOUD.html', 'title': 'HTML metadata for collection', 'type': 'text/html'}, {'rel': 'via', 'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957295-LPCLOUD.json', 'title': 'CMR JSON metadata for collection', 'type': 'application/json'}], 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]}, 'temporal': {'interval': [['2015-11-28T00:00:00.000Z', None]]}}}]}\n",
      "This collection contains All collections provided by LPCLOUD (10 available)\n",
      "\n",
      "\n",
      "HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 has an ID (shortname) of: HLSL30.v2.0\n",
      "HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 has an ID (shortname) of: HLSS30.v2.0\n",
      "spatial: {'bbox': [[-180, -90, 180, 90]]}\n",
      "temporal: {'interval': [['2015-11-28T00:00:00.000Z', None]]}\n",
      "HLS S30 Start Date is: 2015-11-28T00:00:00.000Z\n",
      "spatial: {'bbox': [[-180, -90, 180, 90]]}\n",
      "temporal: {'interval': [['2013-04-11T00:00:00.000Z', None]]}\n"
     ]
    }
   ],
   "source": [
    "# Get a response form the LPCLOUD collection and print the information included in the response\n",
    "lp_collections = [l['href'] for l in lp_links if l['rel'] == 'collections'][0]  # Set collections endpoint to variable\n",
    "print(lp_collections)\n",
    "collections_response = r.get(\"https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/?page=2\", verify=False).json()                        # Call collections endpoint\n",
    "print(collections_response)\n",
    "print(f\"This collection contains {collections_response['description']} ({len(collections_response['collections'])} available)\")\n",
    "\n",
    "collections = collections_response['collections']\n",
    "print(\"\\n\")\n",
    "\n",
    "# Search available version 2 collections for HLS and print them out\n",
    "hls_collections = [c for c in collections if 'HLS' in c['id'] and 'v2' in c['id']]\n",
    "#print(hls_collections)\n",
    "for h in hls_collections: \n",
    "    print(f\"{h['title']} has an ID (shortname) of: {h['id']}\")\n",
    "\n",
    "# Explore the attributes contained in the HLSS30 collection\n",
    "s30 = [h for h in hls_collections if 'HLSS30' in h['id']][0]    # Grab HLSS30 collection\n",
    "\n",
    "for s in s30['extent']:                                         # Check out the extent of this collection\n",
    "    print(f\"{s}: {s30['extent'][s]}\")\n",
    "\n",
    "print(f\"HLS S30 Start Date is: {s30['extent']['temporal']['interval'][0][0]}\")\n",
    "\n",
    "l30 = [h for h in hls_collections if 'HLSL30' in h['id'] and 'v2.0' in h['id']][0]     # Grab HLSL30 collection\n",
    "\n",
    "for l in l30['extent']:                                                                # Check out the extent of this collection\n",
    "    print(f\"{l}: {l30['extent'][l]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b544dee",
   "metadata": {},
   "source": [
    "## 4. STAC Item: Represents data and metadata assets that are spatiotempporally coincident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b93711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID for this item is: HLS.S30.T55JGM.2015332T001732.v2.0\n",
      "It was acquired on: 2015-11-28T00:17:27.450Z\n",
      "over: [148.986215, -25.317703, 148.990385, -25.302755] (Lower Left, Upper Right corner coordinates)\n",
      "It contains 20 assets\n",
      "and is 100% cloudy.\n",
      "Item at index 0 is 100% cloudy.\n",
      "Item at index 1 is 5% cloudy.\n",
      "Item at index 2 is 1% cloudy.\n",
      "Item at index 3 is 2% cloudy.\n",
      "Item at index 4 is 100% cloudy.\n",
      "Item at index 5 is 0% cloudy.\n",
      "Item at index 6 is 100% cloudy.\n",
      "Item at index 7 is 20% cloudy.\n",
      "Item at index 8 is 1% cloudy.\n",
      "Item at index 9 is 0% cloudy.\n",
      "The ID for this item is: HLS.S30.T55JGM.2015332T001732.v2.0\n",
      "It was acquired on: 2015-11-28T00:17:27.450Z\n",
      "over: [148.986215, -25.317703, 148.990385, -25.302755] (Lower Left, Upper Right corner coordinates)\n",
      "It contains 20 assets\n",
      "and is 100% cloudy.\n",
      "The following assets are available for download:\n",
      "B08\n",
      "Fmask\n",
      "B11\n",
      "VZA\n",
      "B02\n",
      "SAA\n",
      "B03\n",
      "B06\n",
      "VAA\n",
      "SZA\n",
      "B07\n",
      "B8A\n",
      "B01\n",
      "B10\n",
      "B04\n",
      "B12\n",
      "B09\n",
      "B05\n",
      "browse\n",
      "metadata\n"
     ]
    }
   ],
   "source": [
    "# Below, go through all links in the collection and return the link containing the items endpoint\n",
    "s30_items = [s['href'] for s in s30['links'] if s['rel'] == 'items'][0]    # Set items endpoint to variable\n",
    "s30_items\n",
    "\n",
    "s30_items_response = r.get(f\"{s30_items}\", verify=False).json()    # Call items endpoint\n",
    "s30_item = s30_items_response['features'][0]         # select first item (10 items returned by default)\n",
    "s30_item\n",
    "\n",
    "# Print metadata attributes from this observation\n",
    "print(f\"The ID for this item is: {s30_item['id']}\")\n",
    "print(f\"It was acquired on: {s30_item['properties']['datetime']}\")\n",
    "print(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\n",
    "print(f\"It contains {len(s30_item['assets'])} assets\")\n",
    "print(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n",
    "\n",
    "for i, s in enumerate(s30_items_response['features']):\n",
    "    print(f\"Item at index {i} is {s['properties']['eo:cloud_cover']}% cloudy.\")\n",
    "\n",
    "item_index = 0    # Indexing starts at 0 in Python, so here select the eighth item in the list at index 0\n",
    "\n",
    "s30_item = s30_items_response['features'][item_index]    # Grab the next item in the list\n",
    "\n",
    "print(f\"The ID for this item is: {s30_item['id']}\")\n",
    "print(f\"It was acquired on: {s30_item['properties']['datetime']}\")\n",
    "print(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\n",
    "print(f\"It contains {len(s30_item['assets'])} assets\")\n",
    "print(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n",
    "\n",
    "print(\"The following assets are available for download:\")\n",
    "for a in s30_item['assets']: print(a)\n",
    "\n",
    "s30_item['assets']['browse']\n",
    "\n",
    "image = io.imread(s30_item['assets']['browse']['href'])    # Load jpg browse image into memory\n",
    "\n",
    "# Basic plot of the image\n",
    "#plt.figure(figsize=(10,10))              \n",
    "#plt.imshow(image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38a739",
   "metadata": {},
   "source": [
    "# Section 2: CMR-STAC API: Searching for Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec512ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables\n",
    "del image, s30_items, s30_items_response, stac_lp, stac_response, l30\n",
    "del a, collections, collections_response, h, hls_collections, l, lp_cloud, lp_collections, s, s30, s30_item\n",
    "\n",
    "lp_search = [l['href'] for l in lp_links if l['rel'] == 'search'][0]    # Define the search endpoint\n",
    "\n",
    "# Set up a dictionary that will be used to POST requests to the search endpoint\n",
    "params = {}\n",
    "\n",
    "search_response = r.post(lp_search, json=params, verify=False).json()    # Send POST request to retrieve items\n",
    "print(f\"{len(search_response['features'])} items found!\")\n",
    "\n",
    "lim = 100\n",
    "params['limit'] = lim  # Add in a limit parameter to retrieve 100 items at a time.\n",
    "print(params)\n",
    "\n",
    "search_response = r.post(lp_search, json=params, verify=False).json()  # send POST request to retrieve first 100 items in the STAC collection\n",
    "\n",
    "print(f\"{len(search_response['features'])} items found!\")\n",
    "\n",
    "# Bring in the farm field region of interest\n",
    "field = gp.read_file('Field_Boundary.geojson')\n",
    "field\n",
    "\n",
    "fieldShape = field['geometry'][0] # Define the geometry as a shapely polygon\n",
    "fieldShape\n",
    "\n",
    "# Use geoviews to combine a basemap with the shapely polygon of our Region of Interest (ROI)\n",
    "#base = gv.tile_sources.EsriImagery.opts(width=500, height=500)\n",
    "#farmField = gv.Polygons(fieldShape).opts(line_color='yellow', color=None)\n",
    "\n",
    "#base * farmField\n",
    "\n",
    "bbox = f'{fieldShape.bounds[0]},{fieldShape.bounds[1]},{fieldShape.bounds[2]},{fieldShape.bounds[3]}'    # Defined from ROI bounds\n",
    "params['bbox'] = bbox                                                                                    # Add ROI to params\n",
    "print(params)\n",
    "\n",
    "search_response = r.post(lp_search, json=params, verify=False).json()    # Send POST request with bbox included\n",
    "print(f\"{len(search_response['features'])} items found!\")\n",
    "\n",
    "date_time = \"2021-07-01T00:00:00Z/2021-08-31T23:59:59Z\"    # Define start time period / end time period\n",
    "params['datetime'] = date_time\n",
    "print(params)\n",
    "\n",
    "search_response = r.post(lp_search, json=params, verify=False).json()    # Send POST request with datetime included\n",
    "print(f\"{len(search_response['features'])} items found!\")\n",
    "\n",
    "s30_id = \"HLSS30.v2.0\"\n",
    "params[\"collections\"] = [s30_id]\n",
    "\n",
    "# Search for the HLSS30 items of interest:\n",
    "s30_items = r.post(lp_search, json=params, verify=False).json()['features']  # Send POST request with collection included\n",
    "len(s30_items)\n",
    "\n",
    "l30_id = \"HLSL30.v2.0\"\n",
    "params[\"collections\"].append(l30_id)\n",
    "print(params)\n",
    "\n",
    "# Search for the HLSS30 and HLSL30 items of interest:\n",
    "hls_items = r.post(lp_search, json=params, verify=False).json()['features']    # Send POST request with S30 and L30 collections included\n",
    "print(len(hls_items))\n",
    "\n",
    "del bbox, date_time, field, lim, lp_links, lp_search, search_response, s30_items    # Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bf0da",
   "metadata": {},
   "source": [
    "# Section 3: Extracting HLS COGs from the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset by Band\n",
    "\n",
    "h = hls_items[0]\n",
    "\n",
    "evi_band_links = []\n",
    "\n",
    "# Define which HLS product is being accessed\n",
    "if h['assets']['browse']['href'].split('/')[4] == 'HLSS30.015':\n",
    "    evi_bands = ['B8A', 'B04', 'B02', 'Fmask'] # NIR RED BLUE Quality for S30\n",
    "else:\n",
    "    evi_bands = ['B05', 'B04', 'B02', 'Fmask'] # NIR RED BLUE Quality for L30\n",
    "\n",
    "# Subset the assets in the item down to only the desired bands\n",
    "for a in h['assets']: \n",
    "    if any(b == a for b in evi_bands):\n",
    "        evi_band_links.append(h['assets'][a]['href'])\n",
    "for e in evi_band_links: print(e)\n",
    "\n",
    "image = io.imread(h['assets']['browse']['href'])  # Load jpg browse image into memory\n",
    "\n",
    "# Basic plot of the image\n",
    "plt.figure(figsize=(10,10))              \n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "del image # Remove the browse image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92b315",
   "metadata": {},
   "source": [
    "# Section 4: Extracting HLS COGs from the \n",
    "\n",
    "## Subset by Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hls_items[0]\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_band_links = []\n",
    "\n",
    "# Define which HLS product is being accessed\n",
    "if h['assets']['browse']['href'].split('/')[4] == 'HLSS30.015':\n",
    "    evi_bands = ['B8A', 'B04', 'B02', 'Fmask'] # NIR RED BLUE Quality for S30 (Sentinel 2)\n",
    "else:\n",
    "    evi_bands = ['B05', 'B04', 'B02', 'Fmask'] # NIR RED BLUE Quality for L30 (Landsat 8)\n",
    "\n",
    "# Subset the assets in the item down to only the desired bands\n",
    "for a in h['assets']: \n",
    "    if any(b == a for b in evi_bands):\n",
    "        evi_band_links.append(h['assets'][a]['href'])\n",
    "for e in evi_band_links: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread(h['assets']['browse']['href'])  # Load jpg browse image into memory\n",
    "\n",
    "# Basic plot of the image\n",
    "plt.figure(figsize=(10,10))              \n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b812ec4",
   "metadata": {},
   "source": [
    "## Load a Spatially Subset HLS COG into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc08b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION CONFIGURATION\n",
    "from netrc import netrc\n",
    "from subprocess import Popen, DEVNULL, STDOUT\n",
    "from getpass import getpass\n",
    "from sys import platform\n",
    "\n",
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine if netrc file exists, and if it includes NASA Earthdata Login Credentials\n",
    "if 'win' in platform:\n",
    "    nrc = '_netrc'\n",
    "else:\n",
    "    nrc = '.netrc'\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(f\"~/{nrc}\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "    del netrcDir\n",
    "\n",
    "# If not, create a netrc file and prompt user for NASA Earthdata Login Username/Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "\n",
    "    # Windows OS won't read the netrc unless this is set\n",
    "    Popen(f'setx HOME {homeDir}', shell=True, stdout=DEVNULL);\n",
    "\n",
    "    if nrc == '.netrc':\n",
    "        Popen(f'touch {homeDir + os.sep}{nrc} | chmod og-rw {homeDir + os.sep}{nrc}', shell=True, stdout=DEVNULL, stderr=STDOUT);\n",
    "\n",
    "    # Unable to use touch/chmod on Windows OS\n",
    "    Popen(f'echo machine {urs} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    Popen(f'echo login {getpass(prompt=prompts[0])} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    Popen(f'echo password {getpass(prompt=prompts[1])} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    del homeDir\n",
    "\n",
    "# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen(f'echo machine {urs} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    Popen(f'echo login {getpass(prompt=prompts[0])} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    Popen(f'echo password {getpass(prompt=prompts[1])} >> {homeDir + os.sep}{nrc}', shell=True)\n",
    "    del homeDir\n",
    "del urs, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "for e in evi_band_links:\n",
    "    print(e)\n",
    "    if e.rsplit('.', 2)[-2] == evi_bands[0]:      # NIR index\n",
    "        nir = rio.open(e)\n",
    "    elif e.rsplit('.', 2)[-2] == evi_bands[1]:    # red index\n",
    "        red = rio.open(e)\n",
    "    elif e.rsplit('.', 2)[-2] == evi_bands[2]:    # blue index\n",
    "        blue = rio.open(e)\n",
    "    elif e.rsplit('.', 2)[-2] == evi_bands[3]:    # Fmask index\n",
    "        fmask = rio.open(e)\n",
    "print(\"The COGs have been loaded into memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)  # Source coordinate system of the ROI\n",
    "utm = pyproj.Proj(nir.crs)                                                  # Destination coordinate system\n",
    "project = pyproj.Transformer.from_proj(geo_CRS, utm)                        # Set up the transformation\n",
    "fsUTM = transform(project.transform, fieldShape)                            # Apply reprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_array, nir_transform = rio.mask.mask(nir, [fsUTM], crop=True)  # Extract the data for the ROI and clip to that bbox\n",
    "\n",
    "plt.imshow(nir_array[0]);  # Quick visual to assure that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_array, _ = rio.mask.mask(red,[fsUTM],crop=True)\n",
    "blue_array, _ = rio.mask.mask(blue,[fsUTM],crop=True)\n",
    "print('Data is loaded into memory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a, e, evi_band_links, evi_bands  # Remove variables that are no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9234ed",
   "metadata": {},
   "source": [
    "# Section 5: Processing HLS Data ------- See if it gets to here\n",
    "\n",
    "## Apply Scale Factor and Calculate EVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700311a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab scale factor from metadata and apply to each band\n",
    "nir_scaled = nir_array[0] * nir.scales[0]\n",
    "red_scaled = red_array[0] * red.scales[0]\n",
    "blue_scaled = blue_array[0] * blue.scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all nodata values to nan\n",
    "nir_scaled[nir_array[0]==nir.nodata] = np.nan \n",
    "red_scaled[red_array[0]==red.nodata] = np.nan \n",
    "blue_scaled[blue_array[0]==blue.nodata] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2308f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evi(red, blue, nir):\n",
    "      return 2.5 * (nir - red) / (nir + 6.0 * red - 7.5 * blue + 1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45664f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_scaled = evi(red_scaled, blue_scaled, nir_scaled) # Generate EVI array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b421ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eviDate = h['properties']['datetime'].split('T')[0]  # Set the observation date to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore matplotlib warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize = (10,7.5))    # Set the figure size (x,y)\n",
    "plt.axis('off')                         # Remove the axes' values\n",
    "\n",
    "fig.suptitle('HLS-derived EVI Data', fontsize=14, fontweight='bold')  # Make a figure title\n",
    "ax = fig.add_subplot(111)                                                                  # Make a subplot\n",
    "ax.set_title(f'Tehama County, CA: {eviDate}', fontsize=12, fontweight='bold')              # Add figure subtitle\n",
    "ax1 = plt.gca()                                                                            # Get current axes\n",
    "\n",
    "# Plot the array, using a colormap and setting a custom linear stretch \n",
    "im = plt.imshow(evi_scaled, vmin=0, vmax=1, cmap='YlGn');\n",
    "\n",
    "# Add a colormap legend\n",
    "plt.colorbar(im, orientation='horizontal', fraction=0.047, pad=0.009, label='EVI', shrink=0.6).outline.set_visible(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9caa3",
   "metadata": {},
   "source": [
    "## Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmask_array, _ = rio.mask.mask(fmask, [fsUTM], crop=True)  # Load in the Quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitword_order = (1, 1, 1, 1, 1, 1, 2)  # set the number of bits per bitword\n",
    "num_bitwords = len(bitword_order)      # Define the number of bitwords based on your input above\n",
    "total_bits = sum(bitword_order)        # Should be 8, 16, or 32 depending on datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qVals = list(np.unique(fmask_array))  # Create a list of unique values that need to be converted to binary and decoded\n",
    "all_bits = list()\n",
    "goodQuality = []\n",
    "for v in qVals:\n",
    "    all_bits = []\n",
    "    bits = total_bits\n",
    "    i = 0\n",
    "\n",
    "    # Convert to binary based on the values and # of bits defined above:\n",
    "    bit_val = format(v, 'b').zfill(bits)\n",
    "    print('\\n' + str(v) + ' = ' + str(bit_val))\n",
    "    all_bits.append(str(v) + ' = ' + str(bit_val))\n",
    "\n",
    "    # Go through & split out the values for each bit word based on input above:\n",
    "    for b in bitword_order:\n",
    "        prev_bit = bits\n",
    "        bits = bits - b\n",
    "        i = i + 1\n",
    "        if i == 1:\n",
    "            bitword = bit_val[bits:]\n",
    "            print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword)) \n",
    "        elif i == num_bitwords:\n",
    "            bitword = bit_val[:prev_bit]\n",
    "            print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "        else:\n",
    "            bitword = bit_val[bits:prev_bit]\n",
    "            print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "\n",
    "    # 2, 4, 5, 6 are the bits used. All 4 should = 0 if no clouds, cloud shadows were present, and pixel is not snow/ice/water\n",
    "    if int(all_bits[2].split(': ')[-1]) + int(all_bits[4].split(': ')[-1]) + \\\n",
    "    int(all_bits[5].split(': ')[-1]) + int(all_bits[6].split(': ')[-1]) == 0:\n",
    "        goodQuality.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce20af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_band = np.ma.MaskedArray(evi_scaled, np.in1d(fmask_array, goodQuality, invert=True))  # Apply QA mask to the EVI data\n",
    "evi_band = np.ma.filled(evi_band, np.nan)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237a133",
   "metadata": {},
   "source": [
    "## Export to COG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalName = nir.name.rsplit('/', 1)[-1]  # Grab the original granule name\n",
    "originalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c67f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "outName = f\"{originalName.split('.B')[0]}_EVI.tif\"  # Generate output name from the original filename\n",
    "tempName = 'temp.tif'                               # Set up temp file\n",
    "outName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output GeoTIFF with overviews\n",
    "evi_tif = rio.open(tempName, 'w', driver='GTiff', height=evi_band.shape[0], width=evi_band.shape[1], count=1,\n",
    "                   dtype=str(evi_band.dtype), crs=nir.crs, transform=nir_transform)\n",
    "\n",
    "evi_tif.write(evi_band, 1)                                    # Write the EVI band to the newly created GeoTIFF\n",
    "evi_tif.build_overviews([2, 4, 8], Resampling.average)        # Calculate overviews\n",
    "evi_tif.update_tags(ns='rio_overview', resampling='average')  # Update tags\n",
    "\n",
    "# Copy the profile, add tiling and compression\n",
    "kwds = evi_tif.profile\n",
    "kwds['tiled'] = True\n",
    "kwds['compress'] = 'LZW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_tif.overviews(1)  # Print overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea22a70",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "evi_tif.close()  # Close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open temp file and export as valid cog\n",
    "with rio.open(tempName, 'r+') as src:\n",
    "    copy(src, outName, copy_src_overviews=True, **kwds)\n",
    "src.close(), os.remove(tempName);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.close(), nir.close(), red.close(), fmask.close(), blue.close()\n",
    "del nir_array, red_array, blue_array, red_scaled, blue_scaled, nir_scaled, i, originalName, outName, prev_bit, qVals, v\n",
    "del all_bits, b, bit_val, bits, bitword, eviDate, evi_band, evi_scaled, fmask_array, goodQuality, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72f3d2",
   "metadata": {},
   "source": [
    "# Section 6: Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba007ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hls_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef82dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put it all together and loop through each of the files, visualize, calculate statistics on EVI, and export\n",
    "for j, h in enumerate(hls_items):\n",
    "    outName = h['assets']['browse']['href'].split('/')[-1].replace('.jpg', '_EVI.tif')\n",
    "\n",
    "    # Check if file already exists in output directory, if yes--skip that file and move to the next observation\n",
    "    if os.path.exists(outName):\n",
    "        print(f\"{outName} has already been processed and is available in this directory, moving to next file.\")\n",
    "        continue\n",
    "    try:\n",
    "        evi_band_links = []\n",
    "        if h['assets']['browse']['href'].split('/')[4] == 'HLSS30.015':\n",
    "            evi_bands = ['B8A', 'B04', 'B02', 'Fmask'] # NIR RED BLUE FMASK\n",
    "        else:\n",
    "            evi_bands = ['B05', 'B04', 'B02', 'Fmask'] # NIR RED BLUE FMASK\n",
    "        for a in h['assets']: \n",
    "            if any(b == a for b in evi_bands):\n",
    "                evi_band_links.append(h['assets'][a]['href'])\n",
    "\n",
    "        # Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "        for e in evi_band_links:\n",
    "            if e.rsplit('.', 2)[-2] == evi_bands[0]: # NIR index\n",
    "                nir = rio.open(e)\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[1]: # red index\n",
    "                red = rio.open(e)\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[2]: # blue index\n",
    "                blue = rio.open(e)\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[3]: # fmask index\n",
    "                fmask = rio.open(e)\n",
    "\n",
    "        # load data and scale\n",
    "        nir_array,nir_transform = rio.mask.mask(nir,[fsUTM],crop=True)\n",
    "        red_array, _ = rio.mask.mask(red,[fsUTM],crop=True)\n",
    "        blue_array, _ = rio.mask.mask(blue,[fsUTM],crop=True)\n",
    "        nir_scaled = nir_array[0] * nir.scales[0]\n",
    "        red_scaled = red_array[0] * red.scales[0]\n",
    "        blue_scaled = blue_array[0] * blue.scales[0]\n",
    "        nir_scaled[nir_array[0]==nir.nodata] = np.nan \n",
    "        red_scaled[red_array[0]==red.nodata] = np.nan \n",
    "        blue_scaled[blue_array[0]==blue.nodata] = np.nan \n",
    "\n",
    "        # Generate EVI\n",
    "        evi_scaled = evi(red_scaled, blue_scaled, nir_scaled)\n",
    "\n",
    "        # Quality Filter the data\n",
    "        fmask_array, _ = rio.mask.mask(fmask,[fsUTM],crop=True)\n",
    "        qVals = list(np.unique(fmask_array))\n",
    "        all_bits = list()\n",
    "        goodQuality = []\n",
    "        for v in qVals:\n",
    "            all_bits = []\n",
    "            bits = total_bits\n",
    "            i = 0\n",
    "            # Convert to binary based on the values and # of bits defined above:\n",
    "            bit_val = format(v, 'b').zfill(bits)\n",
    "            all_bits.append(str(v) + ' = ' + str(bit_val))\n",
    "\n",
    "            # Go through & split out the values for each bit word based on input above:\n",
    "            for b in bitword_order:\n",
    "                prev_bit = bits\n",
    "                bits = bits - b\n",
    "                i = i + 1\n",
    "                if i == 1:\n",
    "                    bitword = bit_val[bits:]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword)) \n",
    "                elif i == num_bitwords:\n",
    "                    bitword = bit_val[:prev_bit]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "                else:\n",
    "                    bitword = bit_val[bits:prev_bit]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "\n",
    "            # 2, 4, 5, 6 are the bits used. All should = 0 if no clouds, cloud shadows were present & pixel is not snow/ice/water\n",
    "            if int(all_bits[2].split(': ')[-1]) + int(all_bits[4].split(': ')[-1]) + \\\n",
    "            int(all_bits[5].split(': ')[-1]) + int(all_bits[6].split(': ')[-1]) == 0:\n",
    "                goodQuality.append(v)\n",
    "        evi_band = np.ma.MaskedArray(evi_scaled, np.in1d(fmask_array, goodQuality, invert=True))  # Apply QA mask to the EVI data\n",
    "        evi_band = np.ma.filled(evi_band, np.nan)\n",
    "\n",
    "        # Remove any observations that are entirely fill value\n",
    "        if np.nansum(evi_band) == 0.0:\n",
    "            print(f\"File: {h['assets']['browse']['href'].split('/')[-1].rsplit('.', 1)[0]} ({h['id']}) was entirely fill values and will not be exported.\")\n",
    "            continue\n",
    "        tempName = \"temp.tif\"\n",
    "        # Create output GeoTIFF with overviews\n",
    "        evi_tif = rio.open(tempName, 'w', driver='GTiff', height=evi_band.shape[0], width=evi_band.shape[1], count=1,\n",
    "                           dtype=str(evi_band.dtype), crs=nir.crs, transform=nir_transform)\n",
    "        evi_tif.write(evi_band, 1)\n",
    "        evi_tif.build_overviews([2, 4, 8], Resampling.average)\n",
    "        evi_tif.update_tags(ns='rio_overview', resampling='average')\n",
    "\n",
    "        # Copy the profile, add tiling and compression\n",
    "        kwds = evi_tif.profile\n",
    "        kwds['tiled'] = True\n",
    "        kwds['compress'] = 'LZW'\n",
    "        evi_tif.close()\n",
    "\n",
    "        # Open temp file and export as valid cog\n",
    "        with rio.open(tempName, 'r+') as src:\n",
    "            copy(src, outName, copy_src_overviews=True, **kwds)\n",
    "        src.close(), os.remove(tempName);\n",
    "\n",
    "    except:\n",
    "        print(f\"Unable to access file: {h['assets']['browse']['href'].split('/')[-1].rsplit('.', 1)[0]} ({h['id']})\")\n",
    "    print(f\"Processing file {j+1} of {len(hls_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cce73d",
   "metadata": {},
   "source": [
    "# Section 7: Stacking HLS Data\n",
    "\n",
    "--- optional -> Section 6 already covers the COG export (what we need most)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
